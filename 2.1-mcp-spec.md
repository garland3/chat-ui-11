# MCP v2.1 Enhanced Specification with Transparent User Storage

This specification extends MCP v2.1 to seamlessly handle large files through user-scoped storage while maintaining security isolation and transparency for both tools and users.

---

## 1. Core Tool Output Contract (Enhanced)

Every MCP tool MUST return a single JSON object. The client automatically manages all file storage transparently to both tools and users.

### 1.1. Top-Level Fields (Enhanced)

| Field | Type | Required | Description |
|---|---|---|---|
| `results` | `object` | **Yes** | The primary, concise result for the model and user. |
| `meta_data` | `object` | No | Additional small, structured facts (metrics, parameters, provenance). |
| `retryable` | `boolean` | No | **NEW**: Indicates if this operation can be safely retried. |
| `artifacts` | `array` | No | Typed file artifacts generated by the tool. |
| `display` | `object` | No | UI hints for controlling the artifact canvas. |
| `deferred_artifacts` | `array` | No | **NEW**: Files created but not returned (for editing workflow). |

---

## 2. User Storage Architecture

### 2.1. Transparent User Storage Area

Each user has a persistent storage area managed by the client:

- **User Perspective**: "My Files" area where all uploaded and generated files appear
- **Technical Implementation**: S3 bucket with user-scoped prefixes
- **Transparency**: Users never see S3 URLs, buckets, or presigned URLs
- **Session Persistence**: Controlled by client feature flag

### 2.2. File Storage Rules and Feature Flags

**Feature Flag: `persist_files_across_sessions`**
- **True (default)**: Files persist in user storage across sessions
- **False**: All user session files deleted when session ends

**Feature Flag: `base64_size_limit_mb`**
- **Default: 300MB**: Files ≤ limit use base64 encoding
- **Configurable**: Client can adjust limit based on performance needs
- Files > limit automatically use S3 storage

**Input Files (User → Tool):**
1. User uploads files through client UI (any size)
2. Client normalizes all filenames for security (see Section 9.1)
3. Client stores files in user's storage area with normalized names
4. Client provides file name references to LLM context
5. Tools receive logical file names, client provides secure local paths

**Output Files (Tool → User):**
1. Tools generate files in `/tmp/{username}/` directory only
2. Client detects generated files and processes based on size
3. Files ≤ `base64_size_limit_mb`: Convert to base64
4. Files > `base64_size_limit_mb`: Upload to S3 storage
5. Files appear in user's storage area automatically

---

## 3. Enhanced File Argument Handling

### 3.1. Automatic File Argument Processing

**Supported File Arguments:**
- `filename` (str): Single file reference
- `filenames` (List[str]): Multiple file references
- **Note**: Only these exact argument names are processed by the client

**Username Injection:**
- Any argument named `username` (str) is automatically overwritten by the client
- The injected username is the authenticated user's identity
- Tools MUST trust this value as authoritative
- Used for file system access control and isolation

**File Resolution Process:**
1. Client identifies `filename`/`filenames` arguments
2. Client resolves file names from user's storage to `/tmp/{username}/input_files/`
3. Client downloads files to secure user-scoped temporary paths
4. Tools receive local file paths: `/tmp/{username}/input_files/actual_filename.ext`

---

## 4. Session Context Enhancement

### 4.1. Automatic File Context Injection

The client MUST automatically augment user prompts with available file information:

**Original User Prompt:**
```
"Analyze my sales data and create a summary report"
```

**Client-Augmented Prompt:**
```
"Analyze my sales data and create a summary report

[SYSTEM: Available files in current session:
- sales_data.csv (uploaded 2024-08-27, 2.1GB, 5M rows)
- customer_segments.xlsx (uploaded 2024-08-25, 15MB)
- previous_analysis.html (generated 2024-08-26, 850KB)
- draft_report.docx (deferred, needs editing)
End available files]"
```

### 4.2. Context Injection Rules

**File Information Format:**
- `filename (source, size, additional_info)`
- `source`: "uploaded YYYY-MM-DD" or "generated YYYY-MM-DD" or "deferred"  
- `size`: Human-readable file size
- `additional_info`: Row counts, dimensions, status, etc.

---

## 5. Security and File System Isolation

### 5.1. Read-Only File System with /tmp Exception

**MCP Server File System Rules:**
- **Read-Only**: Entire file system is read-only except `/tmp`
- **Write Access**: Only `/tmp/{username}/` directory has write permissions
- **Path Prefix**: All file operations MUST use `/tmp/{username}/` prefix
- **Isolation**: Each user's `/tmp/{username}/` directory is isolated

### 5.2. Security Measures

**Filename Normalization:**
- Client MUST normalize all uploaded filenames to prevent directory traversal
- Examples:
  - `"../../malicious.txt"` → `"malicious.txt"`
  - `"..\..\..\windows\system32\file"` → `"file"`
  - `"normal_file.csv"` → `"normal_file.csv"`

**File System Access Control:**
- Server wrapper monkey patches standard file operations
- Write operations restricted to `/tmp/{username}/` only
- Read operations allowed system-wide but write-protected outside `/tmp/{username}/`
- Tools that don't use `username` parameter cannot write files

**Example Secure File Operations:**
```python
# Tool receives username parameter (injected by client)
@mcp.tool()
def process_data(filename: str, username: str) -> dict:
    # Read: Client provides secure path
    df = pd.read_csv(filename)  # /tmp/alice/input_files/data.csv
    
    # Write: Must use /tmp/username/ prefix
    output_path = f"/tmp/{username}/processed_data.parquet"
    df.to_parquet(output_path)
    
    return {
        "artifacts": [{
            "name": "processed_data.parquet",
            "path": output_path,  # Client handles base64 vs S3
            "mime": "application/octet-stream"
        }]
    }
```

---

## 6. Enhanced Artifacts Specification

### 6.1. Artifact Object (Enhanced)

| Field | Type | Required | Description |
|---|---|---|---|
| `name` | `string` | **Yes** | File name (becomes identifier in user storage). |
| `b64` | `string` | No | Base64 content (for files ≤ size limit). |
| `path` | `string` | No | **NEW**: Local file path in `/tmp/{username}/`. |
| `mime` | `string` | **Yes** | The MIME type. |
| `size` | `integer` | No | Size in bytes (auto-populated by client). |
| `description` | `string` | No | Human-readable description. |
| `viewer` | `string` | No | UI hint (`image`, `pdf`, `html`, `code`, `data`). |
| `category` | `string` | No | **NEW**: File category (`report`, `dataset`, `visualization`, `draft`). |
| `auto_open` | `boolean` | No | **NEW**: Open file immediately in viewer. |

### 6.2. Client Processing of Artifacts

**When tool returns `path` field:**
1. Client checks file size at specified path
2. If size ≤ `base64_size_limit_mb`: Convert file to base64, add `b64` field, remove `path`
3. If size > `base64_size_limit_mb`: Upload to S3, add internal S3 reference, remove `path`
4. Client MUST handle both scenarios transparently

**Tool Developer Usage (Path-Based):**
```json
{
  "results": {"summary": "Analysis completed successfully"},
  "artifacts": [{
    "name": "quarterly_report.html",
    "path": "/tmp/alice/quarterly_report.html",
    "mime": "text/html",
    "description": "Q3 financial analysis report",
    "category": "report",
    "auto_open": true
  }]
}
```

**Client Processed Response (Base64, if ≤ size limit):**
```json
{
  "results": {"summary": "Analysis completed successfully"},
  "artifacts": [{
    "name": "quarterly_report.html",
    "b64": "PGh0bWw+PGhlYWQ+...",
    "mime": "text/html",
    "size": 245760,
    "description": "Q3 financial analysis report",
    "category": "report",
    "auto_open": true
  }]
}
```

**Client Processed Response (S3, if > size limit):**
```json
{
  "results": {"summary": "Analysis completed successfully"},
  "artifacts": [{
    "name": "quarterly_report.html",
    "s3_ref": {
      "bucket": "user-storage",
      "key": "alice/artifacts/20240827/quarterly_report.html",
      "internal": true
    },
    "mime": "text/html", 
    "size": 524288000,
    "description": "Q3 financial analysis report",
    "category": "report",
    "auto_open": true
  }]
}
```

---

## 7. Deferred Artifacts (Enhanced)

### 7.1. Deferred Artifact Object

| Field | Type | Required | Description |
|---|---|---|---|
| `name` | `string` | **Yes** | File name (becomes reference in user storage). |
| `path` | `string` | **Yes** | Local path in `/tmp/{username}/`. |
| `mime` | `string` | **Yes** | MIME type. |
| `description` | `string` | No | Description of the deferred file. |
| `reason` | `string` | No | Why deferred (`needs_editing`, `draft`, `incomplete`, `awaiting_approval`). |
| `next_actions` | `array` | No | Suggested next steps for the user. |
| `category` | `string` | No | File category for organization. |
| `expires_hours` | `integer` | No | Hours until file expires (default: 72). |

### 7.2. Deferred Processing

**Client Processing of Deferred Artifacts:**
1. Client stores deferred files in session storage
2. Files remain accessible by name in subsequent tool calls
3. If `persist_files_across_sessions=false`, deferred files deleted on session end
4. Deferred files follow same size-based processing as regular artifacts

---

## 8. Enhanced Display Object

### 8.1. Display Fields (Storage-Aware)

| Field | Type | Default | Description |
|---|---|---|---|
| `open_canvas` | `boolean` | `false` | If true, open artifact in viewer immediately. |
| `primary_file` | `string` | | The `name` of artifact to show by default. |
| `mode` | `string` | | Layout hint (`replace`, `append`, `split`). |
| `storage_action` | `string` | `"store"` | **NEW**: `"store"`, `"store_and_open"`, `"temporary"`. |
| `category_hint` | `string` | | **NEW**: Suggest file categorization. |
| `notification` | `string` | | **NEW**: Message to show when files are stored. |

---

## 9. Client Implementation Requirements

### 9.1. Security Requirements

**Filename Normalization Algorithm:**
```
1. Remove all directory traversal patterns (../, ..\, etc.)
2. Remove absolute path indicators (/, \, C:\, etc.) 
3. Remove null bytes and control characters
4. Limit filename length to reasonable maximum (255 chars)
5. Ensure filename doesn't conflict with system files
6. Preserve file extension for MIME type detection
```

**File System Isolation:**
- Monkey patch `open()`, `os.path`, `pathlib.Path` operations
- Restrict write operations to `/tmp/{username}/` directory only
- Log and block unauthorized file access attempts
- Ensure temporary file cleanup on session end

### 9.2. Username Injection Security

**Username Parameter Processing:**
- Client MUST overwrite any `username` parameter in tool calls
- Value injected is the authenticated user's identity
- Tools can trust this value for access control decisions
- Used for file path isolation: `/tmp/{username}/`

---

## 10. Tool Developer Guidelines

### 10.1. Secure File Operations

**Required Patterns:**
```python
@mcp.tool()
def process_data(filename: str, username: str) -> dict:
    # Input files: Use provided path directly
    data = pd.read_csv(filename)  # Client provides secure path
    
    # Output files: MUST use /tmp/{username}/ prefix
    output_path = f"/tmp/{username}/results.parquet"
    data.to_parquet(output_path)
    
    # Return path for client processing
    return {
        "artifacts": [{
            "name": "results.parquet", 
            "path": output_path,  # Client handles base64 vs S3
            "mime": "application/octet-stream"
        }]
    }
```

**Security Best Practices:**
- Always include `username: str` parameter in tools that write files
- Use `/tmp/{username}/` prefix for all output files
- Never hardcode usernames or attempt file system traversal
- Trust the client-injected username parameter
- Use `path` field in artifacts instead of `b64` for files you create

### 10.2. File Size Unconscious Development

Tools should be written without awareness of file size limits:

```python
@mcp.tool()  
def analyze_dataset(filename: str, username: str) -> dict:
    """Analyze dataset - works with any size file"""
    
    # Normal data processing (file could be 50GB)
    df = pd.read_csv(filename)
    results = df.groupby('category').agg({'sales': 'sum'})
    
    # Save results - client handles size-based routing
    output_path = f"/tmp/{username}/analysis_results.html"
    generate_html_report(results, output_path)
    
    return {
        "results": {"rows_processed": len(df)},
        "artifacts": [{
            "name": "analysis_results.html",
            "path": output_path,  # Client converts to b64 or S3
            "mime": "text/html",
            "description": "Analysis results report"
        }]
    }
```

---

## 11. Tool Response Time Requirements

### 11.1. Response Time Expectations

**Standard Response Time:**
- All MCP tools MUST respond within **10 seconds** under normal conditions
- This includes both successful responses and error responses
- Tools processing large files or complex operations should use progress reporting (see Section 11.2)

### 11.2. Progress Reporting for Long Operations

**Progress Reporting Mechanism:**
- Tools can send progress updates using the MCP Context API
- Progress updates reset the timeout timer
- Recommended for operations that may take longer than 10 seconds

**Example Progress Reporting:**
```python
@mcp.tool()
async def process_large_dataset(filename: str, username: str, ctx: Context = None) -> dict:
    df = pd.read_csv(filename)
    
    if ctx:
        await ctx.info("Starting data processing...")
        await ctx.report_progress(0, 100)
    
    # Long operation with progress updates
    for i, chunk in enumerate(process_chunks(df)):
        # Process chunk...
        if ctx and i % 10 == 0:  # Update every 10 chunks
            progress = (i / total_chunks) * 100
            await ctx.info(f"Processed {i}/{total_chunks} chunks ({progress:.1f}%)")
            await ctx.report_progress(i, total_chunks)
    
    return {"results": {"status": "completed"}}
```

### 11.3. Client Timeout Behavior

**User Notification Schedule:**
- **10 seconds**: No user notification (normal processing time)
- **15 seconds**: First notification - "Tool is taking longer than expected..."
- **20 seconds**: Second notification - "Still processing..."  
- **25 seconds**: Third notification - "Processing continues..."
- **30 seconds**: **TIMEOUT** - Tool execution terminated

**Timeout Termination:**
- Client MUST terminate tool execution after 30 seconds without response or progress
- User receives timeout error notification
- Any partial results are discarded
- Tool process is killed to prevent resource leaks

**User Notification Messages:**
```
15s: "The tool is taking longer than expected. Please wait..."
20s: "Still processing your request. This may take a few more moments."
25s: "Processing continues. The tool will timeout in 5 seconds if no progress."
30s: "Tool failed to respond in a reasonable amount of time. Please try again or use a smaller dataset."
```

---

## 12. Error Handling

### 12.1. Timeout Errors

```json
{
  "results": {
    "error": "Tool execution timed out after 30 seconds"
  },
  "meta_data": {
    "is_error": true,
    "reason": "ExecutionTimeout",
    "error_code": "E_TIMEOUT",
    "details": {
      "timeout_seconds": 30,
      "last_progress": "Processing chunk 45/200",
      "suggestion": "Consider breaking large operations into smaller chunks or using progress reporting"
    },
    "retryable": true
  }
}
```

### 12.2. Security Violation Errors

```json
{
  "results": {
    "error": "File operation outside allowed directory"
  },
  "meta_data": {
    "is_error": true,
    "reason": "SecurityViolation",
    "error_code": "E_INVALID_PATH",
    "details": {
      "attempted_path": "/etc/passwd",
      "allowed_prefix": "/tmp/alice/",
      "suggestion": "Use /tmp/{username}/ prefix for file operations"
    },
    "retryable": false
  }
}
```

### 12.3. File Size Limit Errors

```json
{
  "results": {
    "error": "Generated file exceeds processing limits"
  },
  "meta_data": {
    "is_error": true,
    "reason": "FileSizeExceeded",
    "error_code": "E_FILE_TOO_LARGE",
    "details": {
      "file_size_bytes": 107374182400,
      "current_limit_bytes": 53687091200,
      "suggestion": "Consider generating summary or using chunked processing"
    },
    "retryable": false
  }
}
```

---

This specification ensures secure, isolated file handling with transparent storage management while maintaining simplicity for tool developers and users.